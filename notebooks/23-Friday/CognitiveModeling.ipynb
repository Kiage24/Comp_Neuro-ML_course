{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMkym83QidozXdjS0QdUHFm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"b9F--0OQ148u","executionInfo":{"status":"ok","timestamp":1686779049934,"user_tz":-60,"elapsed":3,"user":{"displayName":"Maria Eckstein","userId":"14689134412796528081"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import plotnine as gg"]},{"cell_type":"markdown","source":["# Part 1: Human Behavior"],"metadata":{"id":"nXQPXqzc5ESu"}},{"cell_type":"markdown","source":["Explain a bit."],"metadata":{"id":"RnF3i-XYCqBz"}},{"cell_type":"code","source":["# Load data (100 subjects of 1 reward schedule)\n","# PROVIDE task explanation figure\n","# PROVIDE: time on x-axis, points for each arm on the y-axis -> insight: reward payoffs change over time\n","# TASK 1a: Describe this figure in words. Can you explain the task based on this figure? What would an optimal strategy look like?\n","# TASK 1b: Plot average choices over x-axis\n","# TASK 1c: What do you see? How do you interpret this finding? -> insight: looks similar to plot above! -> people tend to pick some actions over others"],"metadata":{"id":"JTXcGIea3Z6B","executionInfo":{"status":"ok","timestamp":1686779536529,"user_tz":-60,"elapsed":6,"user":{"displayName":"Maria Eckstein","userId":"14689134412796528081"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Part 2: Train an RL agent to solve the same task"],"metadata":{"id":"rsur2SXV7fLc"}},{"cell_type":"markdown","source":["Explain a bit."],"metadata":{"id":"1ZOD-dF4Cryi"}},{"cell_type":"code","source":["# PROVIDE class for Q-learning agent: random behavioral policy, but calculates Q-values according to Bellman/Q-Update\n","# TASK 2a: Let the agent perform the task (fill in some pieces of code here and there to complete the loop)\n","# TASK 2b: Inspect the behavior (should be random)\n","# TASK 2c: Inspect the value function (should approximate plots above)\n","# TASK 2d: Describe what we have done. (-> Policy Evaluation). What is missing? -> Policy Improvement.\n","# TASK 2e: Implement the policy improvement step (Choose actions according to values)\n","# TASK 2f: Replot behavior (should now look like humans)\n","# TASK 2g: Describe your results in words."],"metadata":{"id":"PIRB_aIu7lWf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Part 3: Use RL as a model for human behavior"],"metadata":{"id":"cL2c4Kc_CHfq"}},{"cell_type":"markdown","source":["We have now trained an RL agent to perform the task. We next want to test if humans might be using RL in a similar way to learn the task. How can we do this?\n","\n","To see if humans use RL to solve the task, we \"fit\" the RL model to human behavior. This means that we \"squeeze\" and \"stretch\" the RL agent until it produces behavior that corresponds to the human behavior.\n","\n","In this case, the \"squeezing\" and \"stretching\" consists of increasing or decreasing the values of the *free parameters* of the model, $\\alpha$ and $\\beta$.\n","\n","How do we know if we need to increase or decrease the values? By checking how close the behavior of the model is to human behavior. The closer the model behavior matches human behavior, the better the model \"fits\" the human dataset. We want the best possible fit, so we are looking for the values of $\\alpha$ and $\\beta$ that *maximizes the probability* that the RL model chooses the same actions that humans have chosen.\n","\n","**In other (more fancy) words, our goal is to find the values for our model parameters ($\\alpha$ and $\\beta$) that maximize the likelihood of the observed (human) behavior under the model.**\n","\n","To do this, we first need to know how likey the observed behavior is under each model. Once we know, all we have to do is to maximize this likelihood.\n","\n","**TASK 1**: Calculate the likelihood of the human dataset under model parameters $\\alpha=0.3$ and $\\beta=3$, by filling in the blanks below.\n","\n","**TASK 2**: Maximize the likelihood of the human dataset by finding the optimal parameters. Fill in the blanks below."],"metadata":{"id":"oLNfbWE5CtPf"}},{"cell_type":"code","source":["# Use the same agent, set alpha and beta to the values above. calculate likelihood for one subject.\n","# Write the loss function: negative log likelihood.\n","# Set up a loop that performs SGD on the loss function.\n","# TASK 3: Simulate behavior from the agent with the fitted parameters.\n","# TASK 3b: Plot the behavior like before. Is it closer to humans?"],"metadata":{"id":"fTSE-belCPFV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Part 4: Use RL as a neural model"],"metadata":{"id":"CupoYbCnII01"}},{"cell_type":"markdown","source":["Like we've seen in the lecture, there is lots of evidence that the brain might implent an RL algorithm: Most notably, the dopamine system has been argued to calculate reward prediction errors (RPEs), such that dopamine neurons *increase* their firing rates when there is a *positive* RPE (reward is *larger* than expected), and *decrease* their firing rates when there is a *negative* RPE (reward is *smaller* than expected).\n","\n","In this section, we will see if this is the case in our dataset.\n","\n","TASK 1: Calculate RPEs. For each trial in the task, calculate the RPE the model is encountering. (Make sure you save the RPEs for each trial so we can later compare them to human striatal activity.)\n","\n","TASK 2: Plot the model-based RPEs against the human fMRI signal (RPEs on the y-axis and BOLD signal on the x-axis). What do you see?\n","\n","TASK 3: Calcualte the correlation between model-based RPEs and striatal BOLD signal. What do you conclude about the hypothesis that the striatal dopamine systems encodes RPEs?"],"metadata":{"id":"CsV3ueFVIPsg"}},{"cell_type":"code","source":["# TASK 4: Calculate RPEs in the model (already done; just need to save)\n","# TASK 4b: Plot\n","# TASK 4c: Calculate correlation"],"metadata":{"id":"GWZQVLuVIFsr","executionInfo":{"status":"ok","timestamp":1686783919597,"user_tz":-60,"elapsed":231,"user":{"displayName":"Maria Eckstein","userId":"14689134412796528081"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Improve the model: Add forgetting"],"metadata":{"id":"UgYhGu5oKJuY"}},{"cell_type":"code","source":[],"metadata":{"id":"P6AK-gnYKOEb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model comparison"],"metadata":{"id":"Kznmn67cKQWG"}},{"cell_type":"code","source":[],"metadata":{"id":"NLKcc464KSRS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Bonus: Fit a neural network to human behavior"],"metadata":{"id":"RSgCu1hpKA6W"}},{"cell_type":"code","source":[],"metadata":{"id":"_ZMyKwwuKAO6","executionInfo":{"status":"ok","timestamp":1686783952812,"user_tz":-60,"elapsed":238,"user":{"displayName":"Maria Eckstein","userId":"14689134412796528081"}}},"execution_count":3,"outputs":[]}]}